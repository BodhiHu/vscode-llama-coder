{
  "name": "llama-coder",
  "displayName": "Llama Coder",
  "description": "Better and self-hosted Github Copilot replacement",
  "version": "0.0.7",
  "icon": "icon.png",
  "publisher": "ex3ndr",
  "repository": {
    "type": "git",
    "url": "https://github.com/ex3ndr/llama-coder.git"
  },
  "bugs": {
    "url": "https://github.com/ex3ndr/llama-coder/issues"
  },
  "license": "MIT",
  "categories": [
    "Machine Learning",
    "Programming Languages"
  ],
  "keywords": [
    "code",
    "assistant",
    "ai",
    "llm",
    "development"
  ],
  "engines": {
    "vscode": "^1.84.0"
  },
  "activationEvents": [
    "onStartupFinished"
  ],
  "main": "./out/extension.js",
  "contributes": {
    "configuration": [
      {
        "title": "Llama coder",
        "properties": {
          "inference.endpoint": {
            "type": "string",
            "default": "http://127.0.0.1:11434/",
            "description": "Ollama Server Endpoint"
          },
          "inference.maxLines": {
            "type": "number",
            "default": 16,
            "description": "Max number of lines to be keep."
          },
          "inference.maxTokens": {
            "type": "number",
            "default": 256,
            "description": "Max number of new tokens to be generated."
          },
          "inference.temperature": {
            "type": "number",
            "default": 0.2,
            "description": "Temperature of the model. Increasing the temperature will make the model answer more creatively."
          },
          "inference.model": {
            "type": "string",
            "enum": [
              "codellama:7b-code-q4_K_S",
              "codellama:7b-code-q4_K_M",
              "codellama:7b-code-q6_K",
              "codellama:7b-code-fp16",
              "codellama:13b-code-q4_K_S",
              "codellama:13b-code-q4_K_M",
              "codellama:13b-code-q6_K",
              "codellama:13b-code-fp16",
              "codellama:34b-code-q4_K_S",
              "codellama:34b-code-q4_K_M",
              "codellama:34b-code-q6_K",
              "deepseek-coder:1.3b-base-q4_0",
              "deepseek-coder:1.3b-base-q4_1",
              "deepseek-coder:1.3b-base-q8_0",
              "deepseek-coder:6.7b-base-q4_K_S",
              "deepseek-coder:6.7b-base-q8_0",
              "deepseek-coder:6.7b-base-fp16",
              "deepseek-coder:33b-base-q4_K_S",
              "deepseek-coder:33b-base-q4_K_M",
              "deepseek-coder:33b-base-fp16"
            ],
            "default": "codellama:7b-code-q4_K_M",
            "description": "Inference model to use"
          }
        }
      }
    ]
  },
  "scripts": {
    "vscode:prepublish": "yarn run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "pretest": "yarn run compile && yarn run lint",
    "lint": "eslint src --ext ts",
    "test": "node ./out/test/runTest.js"
  },
  "devDependencies": {
    "@types/vscode": "^1.84.0",
    "@types/mocha": "^10.0.3",
    "@types/node": "18.x",
    "@typescript-eslint/eslint-plugin": "^6.9.0",
    "@typescript-eslint/parser": "^6.9.0",
    "eslint": "^8.52.0",
    "glob": "^10.3.10",
    "mocha": "^10.2.0",
    "typescript": "^5.2.2",
    "@vscode/test-electron": "^2.3.6"
  }
}